# 🚀 **#DRUGORITHM Hackathon Project**  

**Model Type:** Multimodal Fine-tuned Model (Image + Text → Text)  
**Dataset:** Chest X-rays (Images + Text Descriptions)  
**Task:** Inference on combined image-text input to generate diagnostic/textual output  

### 🔍 **Key Details:**  
- **Input:**  
  - **Image:** Chest X-ray scans  
  - **Text:** Accompanying clinical notes or prompts  
- **Output:** Textual analysis (e.g., diagnoses, observations)  
- **Framework:** Fine-tuned using a vision-language model (SmallVLM2).  

### 🖥 **Gradio Interface**  
A user-friendly interface was developed to demo the model interactively:  
- **Features:**  
  - Upload X-ray images (`PNG/JPG`).  
  - Add optional text prompts (e.g., *"Describe anomalies."*).  
  - Real-time model predictions.  

**Screenshot:**  
![Gradio Interface](https://github.com/AmmarMazen7/DRUGORITHM/blob/main/capture%20test2.png?raw=true)   

### 🛠 **Hackathon Context:**  
Developed during a hackathon to explore multimodal AI for medical imaging, combining:  
- **Computer Vision** (X-ray feature extraction)  
- **NLP** (Generating human-readable insights).  

### 🌟 **Potential Use Cases:**  
- Automated preliminary diagnosis assistance.  
- Educational tool for medical trainees.  
- Efficient triaging in resource-constrained settings.  

*Built with ❤️ during #DRUGORITHM!*  
